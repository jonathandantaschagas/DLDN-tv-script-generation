{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TV Script Generation\n",
    "\n",
    "\n",
    "Neste projeto irei gerar meu próprio script de Os Simpsons utilizando Rede Neural Recorrente (RNN - Recurrent Neural Network). Os dados de treinamento são de 27 temporadas estão disponíveis em  [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data). Como resultado será a Rede Neural irá gerar uma cena para o episódio [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "\n",
    "## Importar os dados\n",
    "\n",
    "Os dados já estão disponiveis. Ele contem uma das cenas e Moe's Tavern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Importando os dados\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "\n",
    "# Ignorando dados do epsódios que não se referem ao script\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exploração dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatísticas do Dataset\n",
      "Quantidade de palavras unicas: 11492\n",
      "Quantidade de cenas: 262\n",
      "Média do número de sentenças em cada cena:15.251908396946565\n",
      "Quantidade de sentenças (linhas): 4258\n",
      "Quantidade média de palavra por sentença: 11.50164396430249\n",
      "\n",
      "Sentencas de 0 a 10:\n",
      "\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "range_sentenca = (0,10)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Estatísticas do Dataset\")\n",
    "print('Quantidade de palavras unicas: {}'.format(len({palavra:None for palavra in text.split()})))\n",
    "cenas = text.split('\\n\\n')\n",
    "print('Quantidade de cenas: {}'.format(len(cenas)))\n",
    "qtd_sentenca_cena = [cena.count('\\n') for cena in cenas]\n",
    "print('Média do número de sentenças em cada cena:{}'.format(np.average(qtd_sentenca_cena)))\n",
    "\n",
    "sentenca = [sentenca for cena in cenas for sentenca in cena.split('\\n')]\n",
    "print('Quantidade de sentenças (linhas): {}'.format(len(sentenca)))\n",
    "\n",
    "qtd_palavras = [len(qtdPalavra.split()) for qtdPalavra in sentenca]\n",
    "print('Quantidade média de palavra por sentença: {}'.format(np.average(qtd_palavras)))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Sentencas de {} a {}:'.format(*range_sentenca))\n",
    "print('\\n'.join(text.split('\\n')[range_sentenca[0]: range_sentenca[1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementar função de preprocessamento\n",
    "A primeira coisa a fazer em qualquer dataset é o preprocessamento.  Implementar as funçoes de preprocessamento abaixo\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "Para criar um word embedding, a primeira coisa a se fazer é tranformar dados em ID, nesta função será criado dois dicionarios:\n",
    "- dicionario que transformaremos de palavras para ID, chamaremos de 'vocab_to_int'\n",
    "- dicionario que transformaremos de ID para palavras, chamaremos de `int_to_vocab`\n",
    "\n",
    "- retornar esses dicionários como uma tupla  `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {word:index for index,word in enumerate(vocab)}\n",
    "    int_to_vocab = {index:word for index,word in enumerate(vocab)}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "    \n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize Punctuation\n",
    "\n",
    "O scrip será quebrado em um array de palabras utilizando os espaços como delimitadoes. Entretando, pontuações como pontos (.) e exclamações(!) tornam a rede neural mais dificil de destinguir algumas palavras como o \"bye\" e \"bye!\".\n",
    "\n",
    "Implementar a função `token_lookup`para retornar um diicionario que será utilizado para tokenizar as pontuações em descrições.\n",
    "Segue abaixo a descrição de cada pontuação\n",
    "\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "Este dicionario será utilizado para tokenizar acentos e adicionar delimitadores. Utilizando esses tokens será mais fácil da rede neural prever qual será a próxima palavra. Para os acentos, será utilizado o pipe (||) para que a rede neurão não confunda como a palavra como parte do dialogo. Ex: ; para ||Semicolon|| e não Semicolon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    return {'.':'||Period||',\n",
    "            ',':'||Comma||',\n",
    "            '\"':'||Quotation_Mark||',\n",
    "            ';':'||Semicolon||', \n",
    "            '!':'||Exclamation_Mark||',\n",
    "            '?':'||Question_Mark||', \n",
    "            '(':'||Left_Parentheses||', \n",
    "            ')':'||Right_Parentheses||',\n",
    "            '--':'||Dash||',\n",
    "            '\\n':'||Return||'}\n",
    "\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessando todos os dados e salvandos\n",
    "Rodando o bloco abaixo todos os dados preprocessado serão salvos (Não será necessários carregalos posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "Para recarregar todos o dados que foram salvos anteriormente basta executar o bloco abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Contruindo a rede neural\n",
    "\n",
    "Para construir a rede neural será necessário a criação das funções abaixo.\n",
    "\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Verifique a versão do Tensorflow e o Acesso a GPU (Caso possua uma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão Tensroflow: 1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: GPU não encontrada. Utilize uma GPU para treinar a Rede Neural\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Verificando versão do tensorflow\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Por favor utilize a versão 1.0 ou mais atual'\n",
    "print('Versão Tensroflow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('GPU não encontrada. Utilize uma GPU para treinar a Rede Neural')\n",
    "else:\n",
    "    print('Dispositivo padrão GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Implementação da função `get_inputs()`para criar os TF Placeholder para a Rede Neural. Devem ser criados os seguintes Placeholders:\n",
    "\n",
    "- Placeholder de entrada do texto chamado 'input'\n",
    "- Placeholder de saída do texto chamado 'target'\n",
    "- Placeholder de taxa de aprendizado chamado 'learning rate'\n",
    "\n",
    "Saiba mais sobre o TF.Placeholder em [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    "\n",
    "A função devera retornar a seguinte tupla de placeholders (Input, Target, LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():    \n",
    "    Input = tf.placeholder(tf.int32, shape=[None, None], name = 'input')\n",
    "    Target = tf.placeholder(tf.int32, shape=[None, None], name='target')\n",
    "    LearningRate = tf.placeholder(tf.float32, name= 'learning_rate')\n",
    "    \n",
    "    return (Input, Target, LearningRate)\n",
    "\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construir e iniciar Célula Rede Neural Recorrente (RNN)\n",
    "Saiba mais em [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) e [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- O tamanho da RNN deve ser definida em `rnn_size`\n",
    "- Inicializar o estado da celula usando MultRNNCell `zero_state()` \n",
    "- Saiba mais em (https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state)\n",
    "- Iniciar o estado inicial da celula com o nome de `initial_state` utilizando o `tf.identity()`\n",
    "- Saiba mais em (https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "- Retornar celula e estado inicia na tupla `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \n",
    "    # Quantidade de Camadados\n",
    "    num_layers = 2\n",
    "    # Celula comun\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    #Multi celulas (2 camadas)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm] * num_layers)\n",
    "    # Iniciando estado inicial da celula\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    # Setando estado da celula\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')    \n",
    "    \n",
    "    return (cell, initial_state)\n",
    "\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Aplicar embedding em `input_data` utilozando Tensoflow.  Retornar embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construir RNN\n",
    "\n",
    "A celula de RNN foi criada na função get_init_cell(). Hora de usar a celula para criar a RNN.\n",
    "\n",
    "- Criar a RNN utilizando [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    "\n",
    "- Definir o estado com o nome \"final_state\" ao final do estado utilizando [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "- Retornar a saída e o estado final da seguindo a tupla `(Outputs, FinalState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    # tf.nn.dynamic_rnn retorna um tensor RNN  e o estado\n",
    "    outputs, initial_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    \n",
    "    initial_state = tf.identity(initial_state, name='final_state')\n",
    "    \n",
    "    return (outputs, initial_state)\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construi a Rede Neural\n",
    "\n",
    "Utilizar a funções implementadas acima para:\n",
    "\n",
    "- Aplicar embedding em `input_data` usando a função `get_embed(input_data, vocab_sizem embed_dim)`\n",
    "\n",
    "- Criar RNN utilizando `cell` e a função `build_rnn(cell, inputs)`\n",
    "\n",
    "- Aplicar uma camada totalmente conectada (fully connected layer) com ativação linear e `vocab_size` com numero de outputs\n",
    "\n",
    "- Retornar os logits e estado final como tupla (Logits, Final_State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    embedding = get_embed(input_data, vocab_size, embed_dim)\n",
    "    (output, final_state) = build_rnn(cell, embedding)\n",
    "    logits = tf.layers.dense(output, vocab_size, activation=None, use_bias=True)\n",
    "    return (logits, final_state)\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batch\n",
    "\n",
    "Implementar `get_batches` para criar batches do input e target utilizando `int_text`\n",
    "\n",
    "Os batches devem sser um array de numpy com o shape `(numero de batch, 2 , tamanho do batch, tamanho da sequencia)`. Cada batch contem dois elementos:\n",
    "\n",
    "- O primeiro elemento é um unico batch do **input** com o shape `[tamanho do batch, tamanho da sequencia]`\n",
    "\n",
    "- O segundo elemento é um unio batch do **target** com o shape `[tamanho do batch, tamanho da sequencia]`\n",
    "\n",
    "Se não for possivel preencher  o ultimo batch com os dados sificientes apagar o ultimo batch.\n",
    "\n",
    "Deverá retornar um array de Numpy conforme abaixo\n",
    "\n",
    "```\n",
    "[\n",
    "  # Primeiro Batch\n",
    "  [\n",
    "    # Batch do Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch do targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Segundo Batch\n",
    "  [\n",
    "    # Batch dk Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch do targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Terceiro Batch\n",
    "  [\n",
    "    # Batch do Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch do targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "Observe que o último valor de destino no último lote é o primeiro valor de entrada do primeiro lote. Neste caso, `1`. Esta é uma técnica comum usada ao criar lotes de seqüência, embora seja bastante insensível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \n",
    "    num_batches = len(int_text) // (batch_size  * seq_length)\n",
    "    \n",
    "    np_text = np.array(int_text[:num_batches * (batch_size  * seq_length)])\n",
    "    \n",
    "    in_text = np_text.reshape(-1, seq_length)\n",
    "    \n",
    "    tar_text = np.roll(np_text, -1).reshape(-1, seq_length)\n",
    "    output = np.zeros(shape=(num_batches, 2, batch_size, seq_length), dtype=np.int)\n",
    "    \n",
    "    # Prepare the output\n",
    "    for idx in range(0, in_text.shape[0]):\n",
    "        i = idx % num_batches\n",
    "        j = idx // num_batches\n",
    "        output[i,0,j,:] = in_text[idx,:]\n",
    "        output[i,1,j,:] = tar_text[idx,:]\n",
    "    return output\n",
    "\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Definindo Hyperparametros\n",
    "\n",
    "- Definir `num_epochs` para numero de epochs\n",
    "- Definir `batch_size` para tamanho do bacth.\n",
    "- Definir `rnn_size` para o tamanho das RNN.\n",
    "- Definir `embed_dim` para tamanho do Embedding.\n",
    "- Definir `seq_length` para tamnho da sequencia.\n",
    "- Definir `learning_rate` para taxa de aprendizagem.\n",
    "- Definir `show_every_n_batches` para printar o progresso de cada batch em treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "rnn_size = 512\n",
    "embed_dim = 256\n",
    "seq_length = 16\n",
    "learning_rate = 0.01\n",
    "show_every_n_batches = 256\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/16   train_loss = 8.823\n",
      "Epoch  16 Batch    0/16   train_loss = 3.029\n",
      "Epoch  32 Batch    0/16   train_loss = 0.958\n",
      "Epoch  48 Batch    0/16   train_loss = 0.311\n",
      "Epoch  64 Batch    0/16   train_loss = 0.204\n",
      "Epoch  80 Batch    0/16   train_loss = 0.198\n",
      "Epoch  96 Batch    0/16   train_loss = 0.196\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pegar Tensores\n",
    "Pegar tensores de `loaded_graph` utilizando a função [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).\n",
    "\n",
    "\n",
    "\n",
    "Pegar os tensores utilizando os nomes :\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Retorne os tensores utilizando a tupla `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    # TODO: Implement Function\n",
    "    input_input = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    \n",
    "    return (input_input, initial_state, final_state, probs )\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Escolher a melhor palavra\n",
    "Implementar função `pick_word` para escolher qual a próxima palavra a ser utilizada na sentença. Utiliznado `probabilities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    pos_word = np.argmax(probabilities)\n",
    "    next_word= int_to_vocab[pos_word]\n",
    "    return next_word\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gerar Script\n",
    "\n",
    "Este bloco irá gerar o script. Definir ´gen_length´ para definir o tamanho do script que será definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak:(looking for approval) heh? heh?\n",
      "moe_szyslak: now, let dr..\n",
      "moe_szyslak: hey, come on, there's sexy bald like...\n",
      "moe_szyslak: oh, oh, what are you doing? you're gettin' some kind of beer.(got up beer) and now, get me to the big cock fight...\n",
      "moe_szyslak:(annoyed) are you all lookin' at?\n",
      "moe_szyslak: you're too late, homer.\n",
      "homer_simpson: yeah, look, i just ordered my own principal, you won't be so...\n",
      "homer_simpson: hey, we all know what i got a job here for your\" best friend...\n",
      "moe_szyslak: yeah, let's go.\n",
      "moe_szyslak: oh, oh, what are you doing? you're gettin' some kind of beer.(got up beer) and now, get me to the big cock fight...\n",
      "moe_szyslak:(annoyed) are you all lookin' at?\n",
      "moe_szyslak: you're too late, homer.\n",
      "homer_simpson: yeah, look, i just ordered my own\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "\n",
    "# homer_simpson, moe_szyslak, ou Barney_Gumble | Pessoas \n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Carregar o modelo salvo\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Pegar os tensores do modelo carregado\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Configuração da eração das sentenças\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Geração das sentenças\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Prever a melhor próxima palavra\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remover os tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# O script gerado está sem sentido ?\n",
    "Está tudo bem se o script de TV não faz sentido. Nós treinamos em menos de um megabyte de texto. Para obter bons resultados, você terá que usar um vocabulário menor ou obter mais dados. Felizmente, há mais dados! Como mencionamos na importação deste projeto, este é um subconjunto de outro conjunto de dados. Não tínhamos treinado em todos os dados, porque isso levaria muito tempo. No entanto, você é livre para treinar sua rede neural em todos os dados. Depois de concluir o projeto, é claro.\n",
    "# Submeter projeto\n",
    "Quando submeter o projeto, certifique-se de ter rodado todas a celulas antes de salvar o notebook. Salve o notebook como \n",
    "\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" e salve ele como arquivo HTML abaixo: \"File  -> \"Download as\". Incluir o arquivo \"helper.py\" e \"problem_unittests.py\" na submissção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
